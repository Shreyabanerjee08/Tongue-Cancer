# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EpxLHl_FtiEerChcPfHJDbo5QT11azCY
"""

!pip install torch_geometric

from email.mime import base
import torch
from torch_geometric.data import Data
import pandas as pd
import numpy as np

data = pd.read_csv("/content/deseq2_tongue.csv", index_col=0)

np.random.seed(314159)

# create positives
label_name = 'training_label'

# find positives
pos_label_col = 'gda_score'
data[pos_label_col].fillna(0, inplace=True)  # Replace NaN values with 0
pos_labels = pd.array([1 if row[pos_label_col] else None for id_, row in data.iterrows()], dtype='Int32')
data[label_name] = pos_labels

# create negatives
def sample_negatives(PU_labels):
    '''randomly samples from the unlabeled samples'''

    # sample same # as positives
    num_pos = (PU_labels==1).sum()
    neg_inds = PU_labels[PU_labels.isna()].sample(num_pos).index

    # TODO: more sophisticated methods for sampling methods. (e.g.: use mutation rate, unsupervised learning, etc.)

    return neg_inds # returns ID's of negative samples

neg_label_inds = sample_negatives(data[label_name])
data[label_name].loc[neg_label_inds] = 0

# TODO: save this data for reproducibility (not now, but once this is finalized and fixed)

data[label_name].value_counts()

data

label_col = 'training_label'
data[label_col] = data[label_col].astype('Int64')

data[label_col].value_counts()

from sklearn.metrics import classification_report

def eval_model(model, X, y):
    preds = model.predict(X)
    clf_report = classification_report(y, preds, labels=[0, 1], target_names=['negative', 'positive'], digits=2)
    print(clf_report)

#FOR NODE
num_node_feats = 100
node_feat_cols = [f'hpa_{i}' for i in range(num_node_feats)]

# get subset of node features features + labels
node_data = data[node_feat_cols + [label_col]]

# restrict to data with labels
node_data_labeled = node_data[node_data[label_col].notna()]
node_data_labeled

# separate features and labels
node_feats = node_data_labeled[node_feat_cols]
node_labels = node_data_labeled[label_col].astype('int32')

# create train-test split

from sklearn.model_selection import train_test_split
test_size = 0.5

X_train,X_test, y_train, y_test= train_test_split(node_feats, node_labels, test_size=test_size, shuffle=True, stratify=node_labels, random_state=42)
# NOTE: train test split is shuffled and stratified across labels

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import StratifiedKFold
# parameter grid to search over
parameters = {
                'n_estimators':[20, 50, 100, 150, 200],
                'criterion':['gini'],
                'max_depth':[1, 2, 3, 4, 5]
             }

# base random forest model
rf = RandomForestClassifier(n_jobs=-1)

# perform a gridsearch with 5-fold crossvalidation to find the best model
rf_clf = GridSearchCV(rf, parameters, n_jobs=-1, refit=True, cv=None)
rf_clf.fit(X_train, y_train)



# show choice of parameters that yielded the best performance
print('Best Parameters')
print(rf_clf.best_params_)
print('\n')

# evaluate model
print('Training Metrics')
eval_model(rf_clf.best_estimator_, X_train, y_train)

print()
print('Testing Metrics')
eval_model(rf_clf.best_estimator_, X_test, y_test)

from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV
from joblib import Parallel, delayed

# parameter grid to search over
parameters = {
    'C': [1, 10, 100],
    'kernel': ['linear', 'rbf'],
    'gamma': ['scale', 'auto']
}

# base SVM model
svm = SVC()

# enable parallel processing using joblib
Parallel(n_jobs=-1)(delayed(svm.fit)(X_train, y_train) for _ in range(5))

# perform a gridsearch with 5-fold crossvalidation to find the best model
svm_clf = GridSearchCV(svm, parameters, cv=5, return_train_score=True)

svm_clf.fit(X_train, y_train)

# show choice of parameters that yielded the best performance
print('Best Parameters')
print(svm_clf.best_params_)
print('\n')

# evaluate model
print('Training Metrics')
eval_model(svm_clf.best_estimator_, X_train, y_train)

print()
print('Testing Metrics')
eval_model(svm_clf.best_estimator_, X_test, y_test)

#KNN
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV

# parameter grid to search over
parameters = {
    'n_neighbors': [20, 50, 100, 150, 200],
    'weights': ['uniform', 'distance'],
    'algorithm': ['ball_tree', 'kd_tree', 'brute']
}

# base KNN model
knn = KNeighborsClassifier()

# perform a gridsearch with 5-fold crossvalidation to find the best model
knn_clf = GridSearchCV(knn, parameters, n_jobs=-1, refit=True, cv=5, return_train_score=True)

knn_clf.fit(X_train, y_train)

# show choice of parameters that yielded the best performance
print('Best Parameters')
print(knn_clf.best_params_)
print('\n')

# evaluate model
print('Training Metrics')
eval_model(knn_clf.best_estimator_, X_train, y_train)

print()
print('Testing Metrics')
eval_model(knn_clf.best_estimator_, X_test, y_test)

#FOR NETWORK
num_network_feats = 128
network_feat_cols = [f'network_{i}' for i in range(num_node_feats)]

# get subset of node features features + labels
network_data = data[network_feat_cols + [label_col]]

# restrict to data with labels
network_data_labeled = network_data[network_data[label_col].notna()]
network_data_labeled

network_feats = network_data_labeled[network_feat_cols]
network_labels = network_data_labeled[label_col].astype('int32')

# create train-test split

from sklearn.model_selection import train_test_split
test_size = 0.25

X_train, X_test, y_train, y_test = train_test_split(network_feats, network_labels, test_size=test_size, shuffle=True, stratify=network_labels)
# NOTE: train test split is shuffled and stratified across labels

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

# parameter grid to search over
parameters = {
                'n_estimators':[20, 50, 100, 150, 200],
                'criterion':['gini'],
                'max_depth':[1, 2, 3, 4, 5]
             }

# base random forest model
rf = RandomForestClassifier(n_jobs=-1)

# perform a gridsearch with 5-fold crossvalidation to find the best model
rf_clf = GridSearchCV(rf, parameters, n_jobs=-1, refit=True, cv=5, return_train_score=True)

rf_clf.fit(X_train, y_train)


# show choice of parameters that yielded the best performance
print('Best Parameters')
print(rf_clf.best_params_)
print('\n')

# evaluate model
print('Training Metrics')
eval_model(rf_clf.best_estimator_, X_train, y_train)

print()
print('Testing Metrics')
eval_model(rf_clf.best_estimator_, X_test, y_test)

from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV
from joblib import Parallel, delayed

# parameter grid to search over
parameters = {
    'C': [1, 10, 100],
    'kernel': ['linear', 'rbf'],
    'gamma': ['scale', 'auto']
}

# base SVM model
svm = SVC()

# enable parallel processing using joblib
Parallel(n_jobs=-1)(delayed(svm.fit)(X_train, y_train) for _ in range(5))

# perform a gridsearch with 5-fold crossvalidation to find the best model
svm_clf = GridSearchCV(svm, parameters, cv=5, return_train_score=True)

svm_clf.fit(X_train, y_train)

# show choice of parameters that yielded the best performance
print('Best Parameters')
print(svm_clf.best_params_)
print('\n')

# evaluate model
print('Training Metrics')
eval_model(svm_clf.best_estimator_, X_train, y_train)

print()
print('Testing Metrics')
eval_model(svm_clf.best_estimator_, X_test, y_test)

#KNN
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV

# parameter grid to search over
parameters = {
    'n_neighbors': [20, 50, 100, 150, 200],
    'weights': ['uniform', 'distance'],
    'algorithm': ['ball_tree', 'kd_tree', 'brute']
}

# base KNN model
knn = KNeighborsClassifier()

# perform a gridsearch with 5-fold crossvalidation to find the best model
knn_clf = GridSearchCV(knn, parameters, n_jobs=-1, refit=True, cv=5, return_train_score=True)

knn_clf.fit(X_train, y_train)

# show choice of parameters that yielded the best performance
print('Best Parameters')
print(knn_clf.best_params_)
print('\n')

# evaluate model
print('Training Metrics')
eval_model(knn_clf.best_estimator_, X_train, y_train)

print()
print('Testing Metrics')
eval_model(knn_clf.best_estimator_, X_test, y_test)

print(X_test, y_test)
num_rows, num_columns = y_test.shape

print(f'The dataset has {num_rows} rows and {num_columns} columns.')

#NODE+NETWORK
node_network_feat_cols = node_feat_cols + network_feat_cols

# get subset of node features features + labels
node_network_data = data[node_network_feat_cols + [label_col]]

# restrict to data with labels
node_network_data_labeled = node_network_data[node_network_data[label_col].notna()]
node_network_data_labeled

# separate features and labels
node_network_feats = node_network_data_labeled[node_network_feat_cols]
node_network_labels = node_network_data_labeled[label_col].astype('int32')

# create train-test split

from sklearn.model_selection import train_test_split
test_size = 0.25

X_train, X_test, y_train, y_test = train_test_split(node_network_feats, node_network_labels, test_size=test_size, shuffle=True, stratify=node_network_labels)
# NOTE: train test split is shuffled and stratified across labels

#RANDOM FOREST
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

# # parameter grid to search over
# parameters = {
#                 'n_estimators':[20, 50, 100, 150, 200],
#                 'criterion':['gini', 'entropy'],
#                 'max_depth':[1, 2, 3, 4, 5]
#              }

# # base random forest model
# rf = RandomForestClassifier(n_jobs=-1)

# # perform a gridsearch with 5-fold crossvalidation to find the best model
# rf_clf = GridSearchCV(rf, parameters, n_jobs=-1, refit=True, cv=5, return_train_score=True)

rf_clf = RandomForestClassifier(n_estimators=150, max_depth=3, criterion='gini', n_jobs=-1)
rf_clf.fit(X_train, y_train)


# # show choice of parameters that yielded the best performance
# print('Best Parameters')
# print(rf_clf.best_params_)
# print('\n')

# evaluate model
print('Training Metrics')
eval_model(rf_clf, X_train, y_train)

print()
print('Testing Metrics')
eval_model(rf_clf, X_test, y_test)

#KNN
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV

# parameter grid to search over
parameters = {
    'n_neighbors': [20, 50, 100, 150, 200],
    'weights': ['uniform', 'distance'],
    'algorithm': ['ball_tree', 'kd_tree', 'brute']
}

# base KNN model
knn = KNeighborsClassifier()

# perform a gridsearch with 5-fold crossvalidation to find the best model
knn_clf = GridSearchCV(knn, parameters, n_jobs=-1, refit=True, cv=5, return_train_score=True)

knn_clf.fit(X_train, y_train)

# show choice of parameters that yielded the best performance
print('Best Parameters')
print(knn_clf.best_params_)
print('\n')

# evaluate model
print('Training Metrics')
eval_model(knn_clf.best_estimator_, X_train, y_train)

print()
print('Testing Metrics')
eval_model(knn_clf.best_estimator_, X_test, y_test)

from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV
from joblib import Parallel, delayed

# parameter grid to search over
parameters = {
    'C': [1, 10, 100],
    'kernel': ['linear', 'rbf'],
    'gamma': ['scale', 'auto']
}

# base SVM model
svm = SVC()

# enable parallel processing using joblib
Parallel(n_jobs=-1)(delayed(svm.fit)(X_train, y_train) for _ in range(5))

# perform a gridsearch with 5-fold crossvalidation to find the best model
svm_clf = GridSearchCV(svm, parameters, cv=5, return_train_score=True)

svm_clf.fit(X_train, y_train)

# show choice of parameters that yielded the best performance
print('Best Parameters')
print(svm_clf.best_params_)
print('\n')

# evaluate model
print('Training Metrics')
eval_model(svm_clf.best_estimator_, X_train, y_train)

print()
print('Testing Metrics')
eval_model(svm_clf.best_estimator_, X_test, y_test)