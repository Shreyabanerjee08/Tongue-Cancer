# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1O6Ywp6v-lwwt9KI2XD45z-lpmxy0Jwrl
"""

pip install pecanpy

import pandas as pd

def ImportDGN():
    dgn = pd.read_csv(r"tongue_gda_summary.csv")
    dgn_dict = pd.read_csv(r"gda_dictionary.csv", index_col=None)

    score_threshold = 0.01
    ei_threshold = 0.5

    dgn = dgn[['Gene', 'EI_gda', 'Score_gda']]
    dgn = dgn.loc[dgn['Score_gda'] >= score_threshold]
    dgn = dgn.loc[dgn['EI_gda'] > ei_threshold]
    dgn.rename({'Score_gda':'gda_score'}, axis=1, inplace=True)
    dgn = dgn.merge(dgn_dict, on="Gene").drop(['Gene'], axis=1)
    dgn['gda_score'] = 1

    return dgn[['ensembl', 'gda_score']]
dgn= ImportDGN()
dgn

import pandas as pd
import re

def import_gdc(file_path):
    gdc = pd.read_csv(file_path)

    feature_name_columns = ['# SSM Affected Cases in Cohort', '# CNV Gain', '# CNV Loss']

    for feature_column in feature_name_columns:
        split_columns = gdc[feature_column].replace({',':''}, regex=True).str.split(' ', n=2, expand=True)
        gdc[f'{feature_column}_1'] = split_columns[0].astype(float)
        gdc[f'{feature_column}_2'] = split_columns[1]

        # Extract numerical part and convert to float
        gdc[f'{feature_column}_3'] = split_columns[2].replace('[^0-9.]', '', regex=True).astype(float)

        gdc[feature_column] = gdc[f'{feature_column}_1'] / gdc[f'{feature_column}_3']
        gdc = gdc.drop([f'{feature_column}_1', f'{feature_column}_2', f'{feature_column}_3'], axis=1)

    gdc.drop(['Symbol', 'Name', 'Cytoband', 'Type', 'Annotations', 'Survival'], axis=1, inplace=True)

    split_columns = gdc['# SSM Affected Cases Across the GDC'].replace({',':''}, regex=True).str.split(' ', n=2, expand=True)
    gdc['1'] = split_columns[0].astype(float)
    gdc['2'] = split_columns[1]

    # Extract numerical part and convert to float
    gdc['3'] = split_columns[2].replace('[^0-9.]', '', regex=True).astype(float)

    gdc['# SSM Affected Cases Across the GDC'] = gdc['1'] / gdc['3']
    gdc = gdc.drop(['1', '2', '3'], axis=1)

    gdc = gdc.rename({'# SSM Affected Cases in Cohort': 'nih_ssm_in_cohort',
                      '# SSM Affected Cases Across the GDC': 'nih_ssm_across_gdc',
                      '# CNV Gain': 'nih_cnv_gain',
                      '# CNV Loss': 'nih_cnv_loss',
                      'Gene ID': 'ensembl',
                      '# Mutations': 'nih_tot_mutations'}, axis=1)

    return gdc

# Example usage:
gdc = import_gdc(r"gdc_tongue_genes.csv")
gdc

import pandas as pd
import numpy as np
from sklearn.decomposition import TruncatedSVD

def ImportHPA():
    hpa = pd.read_csv(r'hpa_tongue_features.csv').drop_duplicates(subset='Gene')

    identifiers = [
        "Gene",
        "Ensembl"
    ]
    discrete_features = [
        "Protein class",
        "Biological process",
        "Molecular function",
        "Disease involvement",
        "Subcellular location",
    ]
    continuous_features = [
        "Tissue RNA - tongue [NX]",
        "Single Cell Type RNA - Mucus-secreting cells [NX]"
    ]

    hpa_features = hpa.iloc[:, hpa.columns.isin(identifiers+discrete_features+continuous_features)]

    hpa_features["Tissue RNA - tongue [NX]"] = (hpa_features["Tissue RNA - tongue [NX]"] - hpa_features["Tissue RNA - tongue [NX]"].mean()) / hpa_features["Tissue RNA - tongue [NX]"].std()
    col = "Single Cell Type RNA - Mucus-secreting cells [NX]"
    hpa_features[col] = (hpa_features[col] - hpa_features[col].mean()) / hpa_features[col].std()

    def explode(feature) :
        return feature.apply(lambda x: x.replace(' ', '').split(','))

    hpa_clean = hpa.fillna('')
    for ft in discrete_features :
        hpa_clean[ft] = explode(hpa_clean[ft])

    protein_class = hpa_clean["Protein class"].explode().unique()
    biological_process = hpa_clean["Biological process"].explode().unique()
    molecular_function = hpa_clean["Molecular function"].explode().unique()
    disease_involvement = hpa_clean["Disease involvement"].explode().unique()
    subcellular_location = hpa_clean["Subcellular location"].explode().unique()
    GO_features = np.concatenate([protein_class, biological_process, molecular_function, disease_involvement, subcellular_location])

    RowFeatures = pd.DataFrame(data = 0,index = hpa_clean['Ensembl'],columns=GO_features)
    counter = 0

    for index, row in RowFeatures.iterrows() :
        features = hpa_clean.iloc[counter][['Protein class', 'Biological process', 'Molecular function', 'Disease involvement', 'Subcellular location']].to_list()
        flattened = [item for sublist in features for item in sublist if item]
        for t in flattened :
            row[t] = 1
        counter +=1

    n_comp = 100
    svd = TruncatedSVD(n_components = n_comp)
    svdModel = svd.fit(RowFeatures)
    visits_emb = svdModel.transform(RowFeatures)
    hpa = pd.DataFrame(data=visits_emb, index=RowFeatures.index).reset_index()
    hpa.columns = ['hpa_' + str(col) for col in hpa.columns]
    hpa = hpa.rename({'hpa_Tissue RNA - tongue [NX]':'nx_tissue_rna_tongue', 'hpa_Single Cell Type RNA - Mucus-secreting cells [NX]':'nx_sing_cell_type_mucus_secreting_cells',
        'hpa_Ensembl':'ensembl'}, axis=1)

    return hpa
hpa = ImportHPA()
hpa

import pandas as pd

def remove_redun(el, verbose=False):
    if verbose:
        print("Original Size: ", len(el))

    el_new = el.iloc[:, 0:2].apply(lambda x: sorted(map(str, x)), axis=1)

    el_new = pd.DataFrame.from_dict(dict(zip(el_new.index, el_new.values))).T

    el_new = el_new.drop_duplicates()
    if verbose:
        postDrop = len(el_new)
        print("After Dropping Duplicates: ", len(el_new), "(-", len(el)-postDrop, ")")

    el_new = el_new.merge(el, left_on=[el_new.columns[0],el_new.columns[1]],  right_on=[el.columns[0], el.columns[1]])
    if verbose:
        print("After Merging: ", len(el_new), "(-", postDrop-len(el_new), ")")
        print()

    return el_new.iloc[:, 2:]

def map_IDs(el, gmap, verbose = False, dropNaNvalues = True):
    gp_map_f = gmap.set_index('#string_protein_id')['alias']

    el_converted = el.reset_index(drop=True)

    el_converted[el_converted.columns[0]] = el_converted[el_converted.columns[0]].map(gp_map_f)
    el_converted[el_converted.columns[1]] = el_converted[el_converted.columns[1]].map(gp_map_f)

    if verbose:
        print("NaN values per Column:", el_converted[el_converted.columns[0]].isna().sum(), el_converted[el_converted.columns[1]].isna().sum())

    if dropNaNvalues:
        el_converted = el_converted.dropna()
        if verbose:
            print("New edge list size:", len(el_converted), "( -", len(el)-len(el_converted), ")")

    return el_converted

def ImportSTRING():
    el_map = pd.read_csv(r'9606.protein.aliases.v12.0.txt', sep="\t")
    el = pd.read_csv(r'9606.protein.links.detailed.v12.0.txt', sep=" ")
    el_map = el_map.loc[el_map.source == 'Ensembl_gene']

    el = remove_redun(el, True)
    el = map_IDs(el, el_map, verbose=True)

    return el

strg = ImportSTRING()
strg

master = hpa.merge(gdc, on="ensembl")
master

strg_allgenes = pd.concat([strg['protein1'], strg['protein2']])
strg_allgenes = strg_allgenes.drop_duplicates()
master = master.loc[master['ensembl'].isin(strg_allgenes)]
master

strg_intersect = strg.iloc[:, :2].merge(master["ensembl"], right_on="ensembl", left_on='protein1').drop("ensembl", axis=1)
strg_intersect = strg_intersect.merge(master["ensembl"], right_on="ensembl", left_on='protein2').drop("ensembl", axis=1).rename(columns={'protein1':'gene1', 'protein2':'gene2'})
strg = strg_intersect.merge(strg, right_on=['protein1', 'protein2'], left_on=['gene1', 'gene2']).drop(['protein1','protein2'], axis=1)
strg

strg[['gene1', 'gene2']].to_csv(r'tongue_edge_list.npy', index=False, header=False)
strg.to_csv(r'tongue_edge_list_features.npy', index=False)