# -*- coding: utf-8 -*-
"""Untitled16new.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KEJKJj70F0n64gAowYgc1bjntkvFBbvm
"""

!pip install wandb

!pip install torch_geometric

!pip install pytorch_lightning

import os
os.chdir('E:/genedragnn1')
cwd = os.getcwd()
cwd

import numpy as np
import pandas as pd

np.random.seed(314159) # set random seed

import torch
import pytorch_lightning as pl

from torch_geometric.data import Data

import wandb

# load edge list
edge_list_path = 'edge_list(1).npy'
edge_list = torch.Tensor(np.load(edge_list_path).T).type(torch.int64) # read in format expected by pytorch geometric [2, n_edges]

# load protein-ID dictionary (need new ID system starting at index 0 for pytorch geometric)
protein_id_dict = np.load('protein_ids_dict.npy', allow_pickle=True).item() # maps my custom ID system to Ensembl IDs
protein_id_dict_inv = {Ensembl: id_ for id_, Ensembl in protein_id_dict.items()} # maps Ensembl IDs to my custom ID system

data_path = 'node_dataset-Copy1.csv' #NOTE: labels are generated from infomation in this dataset
node_dataset = pd.read_csv(data_path, index_col=0)

# map dataset
myID = node_dataset.index.map(protein_id_dict_inv).rename('myID')
node_dataset.insert(loc=0, column='myID', value=myID)
node_dataset = node_dataset.reset_index().set_index('myID')

# ensure that the index is in the correct order
node_dataset.sort_index(inplace=True)

# reset the index to the default integer index
node_dataset.reset_index(drop=True, inplace=True)

assert((node_dataset.index.to_numpy() == np.arange(len(node_dataset))).all())

# create positives
label_name = 'my_label'

# find positives
pos_label_col = 'gda_score' #FIXME: figure out meaning of columns and determing appropriate choice of positive labels
node_dataset[pos_label_col].fillna(0, inplace=True) # Replace NaN values with 0  # Replace NaN values with 0
pos_labels = pd.array([1 if row[pos_label_col] else None for id_, row in node_dataset.iterrows()], dtype='Int32')
node_dataset[label_name] = pos_labels

# create negatives
def sample_negatives(PU_labels):
    '''randomly samples from the unlabeled samples'''

    # sample same # as positives
    num_pos = (PU_labels==1).sum()
    neg_inds = PU_labels[PU_labels.isna()].sample(num_pos).index

    # TODO: more sophisticated methods for sampling methods. (e.g.: use mutation rate, unsupervised learning, etc.)

    return neg_inds # returns ID's of negative samples

neg_label_inds = sample_negatives(node_dataset[label_name])
node_dataset.loc[neg_label_inds, label_name] = 0

# TODO: save this data for reproducibility (not now, but once this is finalized and fixed)

node_dataset[label_name].value_counts()

node_dataset
node_dataset.to_csv("nodedataset.csv", index=True)

label_col = label_name
node_dataset[label_col] = node_dataset[label_col].astype('Int32')

# TODO: decide whether or not to include network embedding features...
num_node_feats = 100
node_feat_cols = [f'hpa_{i}' for i in range(num_node_feats)]

# get subset of node features features + labels
node_data = node_dataset[node_feat_cols + [label_col]]

X = torch.Tensor(node_data[node_feat_cols].to_numpy())#.type(torch.float64)

y = node_data[label_col].fillna(-1).astype('int') # fill NaN with -1 so that it can be converted to pytorch tensor
y = torch.Tensor(y).type(torch.int64)

# restrict to data with labels
node_data_labeled = node_data[node_data[label_col].notna()]
node_data.to_csv("nodedata.csv", index=False)

0.1 * (1/(1-0.2))

from sklearn.model_selection import train_test_split

X_myIDs = node_data_labeled.index.to_numpy() # myIDs for nodes with labels for training/testing
labels = node_data_labeled[label_col].to_numpy() # for stratification

test_size = 0.2
val_size = 0.1 * (1/(1-test_size))

myIDs_train_val, myIDs_test = train_test_split(X_myIDs, test_size=test_size, shuffle=True, stratify=labels)

labels_train_val = node_data_labeled.loc[myIDs_train_val][label_col].to_numpy()
myIDs_train, myIDs_val = train_test_split(myIDs_train_val, test_size=val_size, shuffle=True, stratify=labels_train_val)

# NOTE: train-val-test split is shuffled and stratified
# TODO: look into any special consideration necessary for train-test splits on graph-based models

# create masks
n_nodes = len(node_data)
train_mask = np.zeros(n_nodes, dtype=bool)
train_mask[myIDs_train] = True
train_mask = torch.Tensor(train_mask).type(torch.bool)

val_mask = np.zeros(n_nodes, dtype=bool)
val_mask[myIDs_val] = True
val_mask = torch.Tensor(val_mask).type(torch.bool)

test_mask = np.zeros(n_nodes, dtype=bool)
test_mask[myIDs_test] = True
test_mask = torch.Tensor(test_mask).type(torch.bool)

data = Data(x=X, y=y, edge_index=edge_list)
num_classes = 2
num_features = X.shape[1]

data.train_mask = train_mask
data.val_mask = val_mask
data.test_mask = test_mask
data

from torch_geometric.nn import GCNConv, GATConv
import torch.nn.functional as F

# define GNN architecture
class GNNModel(torch.nn.Module):
    def __init__(self, num_features, hidden_channels, num_classes, hidden_dense, GNN_conv_layer=GCNConv, dropout_rate=0.1, **kwargs):
        """
        Args:
            num_features (int): Dimension of input features
            hidden_channels (List[int]): Dimension of hidden features
            num_classes (int): Dimension of the output.
            hidden_dense (int): number of units in hidden dense layer following convolutions.
            GNN_conv_layer: Class of the graph convolutional layer to use.
            dropout_rate (float): Dropout rate to apply throughout the network
            kwargs: Additional arguments for the graph layer (e.g. number of heads for GAT)
        """
        super().__init__()

        self.convs = []
        self.convs.append(GNN_conv_layer(in_channels=num_features, out_channels=hidden_channels[0], **kwargs)) # first GNN Conv layer

        for c1, c2 in zip(hidden_channels[:-1], hidden_channels[1:]): # middle layers
            self.convs.append(GNN_conv_layer(in_channels=c1, out_channels=c2, **kwargs))

        self.convs = torch.nn.ModuleList(self.convs)

        self.dense1 = torch.nn.Linear(hidden_channels[-1], hidden_dense)
        self.dense_out = torch.nn.Linear(hidden_dense, num_classes)

        self.dropout_rate = dropout_rate

    def forward(self, x, edge_index):
        """
        Args:
            x: node features
            edge_index: edge list
        """

        for i,conv in enumerate(self.convs):
            x = conv(x, edge_index)
            x = x.relu()
            x = F.dropout(x, p=self.dropout_rate, training=self.training)

        x = self.dense1(x)
        x = x.relu()
        x = F.dropout(x, p=self.dropout_rate, training=self.training)
        x = self.dense_out(x)

        return x
import pytorch_lightning as pl

# define Pytorch Lightning model
class LitGNN(pl.LightningModule):
    def __init__(self, model_name, **model_kwargs):
        super().__init__()
        # Saving hyperparameters
        self.save_hyperparameters()
        self.validation_step_outputs = []

        self.model_name = model_name
        self.model = GNNModel(**model_kwargs)
        self.loss_module = torch.nn.CrossEntropyLoss()

        self.example_input_array = data

    def forward(self, data, mode="train"):
        x, edge_index = data.x, data.edge_index
        x = self.model(x, edge_index)

        # Only calculate the loss and acc on the nodes corresponding to the mask
        if mode == "train":
            mask = data.train_mask
        elif mode == "val":
            mask = data.val_mask
        elif mode == "test":
            mask = data.test_mask
        else:
            raise ValueError(f"Unknown forward mode: {mode}")

        #TODO: add other metrics like recall, precision, f1, etc...
        loss = self.loss_module(x[mask], data.y[mask])
        acc = (x[mask].argmax(dim=-1) == data.y[mask]).sum().float() / mask.sum()

        return x, loss, acc

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters())#SGD(self.parameters(), lr=0.1, momentum=0.9, weight_decay=2e-3)
        return optimizer

    def training_step(self, batch, batch_idx):
        x, loss, acc = self.forward(batch, mode="train")
        self.log("train_loss", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)
        self.log("train_acc", acc, on_step=True, on_epoch=True, prog_bar=True, logger=True)
        return loss

    def validation_step(self, batch, batch_idx):
        logits, _, acc = self.forward(batch, mode="val")
        self.log("val_acc", acc, on_step=True, on_epoch=True, prog_bar=True, logger=True)
        self.validation_step_outputs.append(logits)
        return logits

    def on_validation_epoch_end(self):
        # NOTE: can't save non-standard GNN model like this
        # TODO: look into how to save torch geometric models
        # dummy_input = data
        # model_filename = f'{self.model_name}_{str(self.global_step).zfill(5)}.onnx'
        # torch.onnx.export(self, dummy_input, model_filename)
        # wandb.save(model_filename)


        epoch_average = torch.stack(self.validation_step_outputs).mean()
        self.log("validation_epoch_average", epoch_average)
        self.validation_step_outputs.clear()  # free memory

    def test_step(self, batch, batch_idx):
        x, _, acc = self.forward(batch, mode="test")
        self.log("test_acc", acc, on_step=True, on_epoch=True, prog_bar=True, logger=True)

    # def test_epoch_end(self, test_step_outputs):
    #     # save model as onnx format
    #     pass

import wandb
import os

os.environ["WANDB_API_KEY"] = "c637accdcb6b63eb7bfb796aff4eba31dd8d275c"
os.environ["WANDB_MODE"] = "offline"

config = {
    "dataset": "CIFAR10",
    "machine": "offline cluster",
    "model": "CNN",
    "learning_rate": 0.01,
    "batch_size": 128,
}

wandb.init(project="offline-demo")

for i in range(100):
    wandb.log({"accuracy": i})

from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping
from pytorch_lightning.loggers import TensorBoardLogger, WandbLogger
import torch_geometric.loader

import datetime

model_name = f'gat_{str(datetime.datetime.today())[:10]}'

logger = WandbLogger(name=model_name, project="Project X", log_model="all")


AVAIL_GPUS = min(1, torch.cuda.device_count())

model = LitGNN(model_name, num_features=num_features, hidden_channels=[128],
               num_classes=num_classes, hidden_dense=64, GNN_conv_layer=GCNConv, dropout_rate=0.1)

data_loader = torch_geometric.loader.DataLoader([data], batch_size=1, num_workers=3)
MAX_EPOCHS=200
trainer = pl.Trainer(
    callbacks=[ModelCheckpoint(save_weights_only=False, mode="max", monitor="val_acc"),
               EarlyStopping(monitor="val_acc_epoch", patience=100, verbose=False, mode="max")],
    accelerator='cpu',
    max_epochs=MAX_EPOCHS,
    logger=logger,
)

trainer.fit(model, data_loader, data_loader)
model = LitGNN.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)

def save_reports(filename, train_reports, test_reports):
    '''saves train and test reports to a json file'''
    if not os.path.exists('project_reports'):
       os.makedirs('project_reports')
    save_dict = {'train_reports': train_reports, "test_reports": test_reports}
    json_string = json.dumps(save_dict)
    json_file = open(f'{filename}.json', 'w')
    json_file.write(json_string)
    json_file.close()

# evaluate model

from sklearn.metrics import classification_report
logits, _, _ = model.forward(data.to(device='cpu'))

preds_train = logits[data.train_mask].argmax(dim=-1)
preds_test = logits[data.test_mask].argmax(dim=-1)

y_train = data.y[data.train_mask]
y_test = data.y[data.test_mask]

train_report = classification_report(y_train, preds_train, labels=[0,1], target_names=['negative', 'positive'])
test_report = classification_report(y_test, preds_test, labels=[0,1], target_names=['negative', 'positive'])

print('training metrics')
print(train_report)
print()
print('testing metrics')
print(test_report)

# evaluate model

from sklearn.metrics import classification_report
logits, _, _ = model.forward(data.to(device='cpu'))

preds_train = logits[data.train_mask].argmax(dim=-1)
preds_test = logits[data.test_mask].argmax(dim=-1)

y_train = data.y[data.train_mask]
y_test = data.y[data.test_mask]

train_report = classification_report(y_train, preds_train, labels=[0,1], target_names=['negative', 'positive'])
test_report = classification_report(y_test, preds_test, labels=[0,1], target_names=['negative', 'positive'])

print('training metrics')
print(train_report)
print()
print('testing metrics')
print(test_report)

np.save(r"GNN_test")

test=data.train_mask.numpy()
np.savetxt("train_GNN1.csv", test, fmt='%d', delimiter=',')

train=y_train.numpy()
np.savetxt("actual_train_GNN.csv", train, fmt='%d', delimiter=',')

y_test

preds_test=

true = torch.nonzero(preds_test).squeeze()
true

test_data_indices = torch.arange(len(data.y))[data.test_mask]
pos_test_mask = preds_test == 1
preds_test_indices = torch.arange(len(data.y))[data.test_mask][pos_test_mask]

test_indices_and_preds = torch.cat([test_data_indices[pos_test_mask].unsqueeze(1), preds_test_indices.unsqueeze(1)], dim=1)

test_indices_and_preds

preds_test_indices

test_data_indices = torch.arange(len(data.y))[data.test_mask]
preds_test_indices = torch.arange(len(data.y))[data.test_mask][preds_test == 1]  # assuming 1 is the positive class
preds_test_indices

test_indices = data.test_mask.nonzero(as_tuple=False).squeeze()

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, roc_auc_score
import torch
import torch_geometric
from torch_geometric.data import DataLoader
from tqdm import tqdm
import numpy as np
import torch
import pytorch_lightning as pl
from pytorch_lightning.loggers import WandbLogger
from pytorch_lightning.callbacks import ModelCheckpoint
from sklearn.model_selection import train_test_split

# Compute true positive rate and false positive rate for all thresholds
fpr, tpr, thresholds = roc_curve(data.y[data.test_mask], logits[data.test_mask][:, 1].detach().numpy())

# Plot ROC curve
plt.plot(fpr, tpr, label='ROC curve')
plt.plot([0, 1], [0, 1], 'k--', label='Random guess')
plt.xlabel('False positive rate')
plt.ylabel('True positive rate')
plt.title('Receiver operating characteristic (ROC) curve_GNN')
plt.legend()

# Compute AUC
auc = roc_auc_score(data.y[data.test_mask], logits[data.test_mask][:, 1].detach().numpy())

# Add AUC value to plot
plt.text(1.0, 0.5, f'AUC: {auc:.4f}', transform=plt.gca().transAxes, ha='right', va='center')

# Display plot
plt.show()

# Save plot as TIFF file
plt.savefig('roc_curve.tiff', dpi=1200, format='tif')

# Get gene IDs and GNN scores
gene_ids = node_data.index.to_numpy()
gnn_scores_negative = logits[:, 0].detach().numpy()
gnn_scores_positive = logits[:, 1].detach().numpy()

# Zip gene IDs and GNN scores together
gene_gnn_scores_negative = list(zip(gene_ids, gnn_scores_negative))
gene_gnn_scores_positive = list(zip(gene_ids, gnn_scores_positive))

# Sort genes by GNN score in descending order
sorted_gene_gnn_scores_negative = sorted(gene_gnn_scores_negative, key=lambda x: x[1], reverse=True)
sorted_gene_gnn_scores_positive = sorted(gene_gnn_scores_positive, key=lambda x: x[1], reverse=False)

# Perform Wilcoxon signed-rank test for the positive label
from scipy.stats import wilcoxon
w, p = wilcoxon(gnn_scores_negative, gnn_scores_positive)

# Print top 10 genes by GNN score for the negative label
print("Top 10 genes by negative GNN score:")
for i, (gene_id, gnn_score) in enumerate(sorted_gene_gnn_scores_negative[:10]):
    print(f"{i+1}. {gene_id}: {gnn_score:.4f}")

# Print top 10 genes by GNN score for the positive label
print("Top 10 genes by positive GNN score:")
for i, (gene_id, gnn_score) in enumerate(sorted_gene_gnn_scores_positive[:10]):
    print(f"{i+1}. {gene_id}: {gnn_score:.4f} (p-value: {p:.4f})")

# Save the output to a file
with open("Negative GNN", "w") as f:
    f.write("Top 10 genes by negative GNN score:\n")
    for i, (gene_id, gnn_score) in enumerate(sorted_gene_gnn_scores_negative[:]):
        f.write(f"{i+1}. {gene_id}: {gnn_score:.4f}\n")

    f.write("\nTop 10 genes by positive GNN score:\n")
    for i, (gene_id, gnn_score) in enumerate(sorted_gene_gnn_scores_positive[:10]):
        f.write(f"{i+1}. {gene_id}: {gnn_score:.4f} (p-value: {p:.4f})\n")

# Get gene IDs and GNN probabilities
gene_ids = node_data.index.to_numpy()
gnn_probs_negative = torch.nn.softmax(logits[:, 0], dim=0).detach().numpy()
gnn_probs_positive = torch.nn.softmax(logits[:, 1], dim=0).detach().numpy()

# Zip gene IDs and GNN probabilities together
gene_gnn_probs_negative = list(zip(gene_ids, gnn_probs_negative))
gene_gnn_probs_positive = list(zip(gene_ids, gnn_probs_positive))

# Sort genes by GNN probability in descending order
sorted_gene_gnn_probs_negative = sorted(gene_gnn_probs_negative, key=lambda x: x[1], reverse=True)
sorted_gene_gnn_probs_positive = sorted(gene_gnn_probs_positive, key=lambda x: x[1], reverse=False)

# Perform Wilcoxon signed-rank test for the positive label
from scipy.stats import wilcoxon
w, p = wilcoxon(gnn_probs_negative, gnn_probs_positive)

# Print top 10 genes by GNN probability for the negative label
print("Top 10 genes by negative GNN probability:")
for i, (gene_id, gnn_prob) in enumerate(sorted_gene_gnn_probs_negative[:10]):
    print(f"{i+1}. {gene_id}: {gnn_prob:.4f}")

# Print top 10 genes by GNN probability for the positive label
print("Top 10 genes by positive GNN probability:")
for i, (gene_id, gnn_prob) in enumerate(sorted_gene_gnn_probs_positive[:10]):
    print(f"{i+1}. {gene_id}: {gnn_prob:.4f} (p-value: {p:.4f})")

# Save the output to a file
with open("Negative GNN", "w") as f:
    f.write("Top 10 genes by negative GNN probability:\n")
    for i, (gene_id, gnn_prob) in enumerate(sorted_gene_gnn_probs_negative[:]):
        f.write(f"{i+1}. {gene_id}: {gnn_prob:.4f}\n")

    f.write("\nTop 10 genes by positive GNN probability:\n")
    for i, (gene_id, gnn_prob) in enumerate(sorted_gene_gnn_probs_positive[:10]):
        f.write(f"{i+1}. {gene_id}: {gnn_prob:.4f} (p-value: {p:.4f})\n")

np_array = gene_gnn_scores_negative
np.savetxt("negative_gnn1.csv", np_array, fmt="%f", delimiter=",")



probabilities = F.sigmoid(logits)
gene_ids = node_data.index.to_numpy()
# Extract probabilities for class 0 and class 1
probabilities_class_0 = probabilities[:, 0]
probabilities_class_1 = probabilities[:, 1]
gnn_neg=list(zip(gene_ids,probabilities_class0 ))
gnn_pos=list(zip(gene_ids,probabilities_class1 ))

probabilities_class0 = probabilities_class_0.tolist()
probabilities_class1 = probabilities_class_1.tolist()

np_array = gnn_neg
np.savetxt("negative_gnn2.csv", np_array, fmt="%f", delimiter=",")

np_array = gnn_pos
np.savetxt("positive_gnn2.csv", np_array, fmt="%f", delimiter=",")

probabilities_class1

import numpy as np
from scipy.stats import wilcoxon
logits_np = logits.detach().numpy()

# Initialize an array to store the p-values
p_values = np.zeros(logits_np.shape[0])

# Loop over each gene
for i in range(logits_np.shape[0]):
    # Extract the logits for the current gene
    gene_logits = logits_np[i,:]

    # Calculate the p-value using the Wilcoxon signed-rank test
    # We assume that the null hypothesis is that the logits come from a distribution with zero location parameter
    # If you want to test a different null hypothesis, you can modify the 'alternative' parameter
    _, p_value = wilcoxon(gene_logits, zero_method='pratt')

    # Store the p-value for the current gene
    p_values[i] = p_value

print("P-values for each gene:", p_values)

print(logits_np.shape)